{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fHhtIP8ngIi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import nltk,re,string\n",
        "from nltk.corpus import stopwords, twitter_samples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "  stemmer=nltk.PorterStemmer()\n",
        "  stopwords_english=stopwords.words('english')\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "  tokenizer=nltk.TweetTokenizer(preserve_case=False,strip_handles=True,reduce_len=True)\n",
        "  tweet_tokens=tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweets_clean=[]\n",
        "  for word in tweet_tokens:\n",
        "    if(word not in stopwords_english and word not in string.punctuation):\n",
        "      stem_word=stemmer.stem(word) #stemming a word\n",
        "      tweets_clean.append(stem_word)\n",
        "\n",
        "  return tweets_clean\n"
      ],
      "metadata": {
        "id": "rjPXiTTbn8Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets,ys):\n",
        "  ##input:\n",
        "  ##tweets:a list of tweets\n",
        "  ##ys: a m x 1 arraywith the sentiment label of each tweet as 0 or 1\n",
        "  ##output:\n",
        "  ##freqs: a dictionary mapping each (word,sentiment) pair to its frequency\n",
        "  yslist=np.squeeze(ys).tolist()\n",
        "  freqs={}\n",
        "  for y, tweet in zip(yslist,tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair=(word,y)\n",
        "      if pair in freqs:\n",
        "        freqs[pair]=freqs[pair]+1\n",
        "      else:\n",
        "        freqs[pair]=1\n",
        "  return freqs"
      ],
      "metadata": {
        "id": "MhUfls35rYZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD_1TIUKuK21",
        "outputId": "79ace1b9-11fd-4f84-b71e-1b5ac9d0478a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets=['i am happy','i am tricked','i am sad','i am tired','i am tired']\n",
        "ys=[1,0,0,0,0]\n",
        "res=build_freqs(tweets,ys)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdAa9TwRtWN3",
        "outputId": "9adcc51a-7a12-4562-d2d1-fba1a2ad78df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_positive_tweets=twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets=twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "hYSkVkPPuAzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos=all_positive_tweets[4000:]\n",
        "train_pos=all_positive_tweets[:4000]\n",
        "test_neg=all_negative_tweets[4000:]\n",
        "train_neg=all_negative_tweets[4000:]"
      ],
      "metadata": {
        "id": "-IISO3FFvC4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x=train_pos+train_neg #x label\n",
        "test_x=test_pos+test_neg"
      ],
      "metadata": {
        "id": "YmgqfRK6wS-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y=np.append(np.ones((len(train_pos),1)),np.zeros((len(train_neg),1)),axis=0) #y label\n",
        "test_y=np.append(np.ones((len(test_pos),1)),np.zeros((len(test_neg),1)),axis=0)"
      ],
      "metadata": {
        "id": "qKaNU7dsyY4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freqs=build_freqs(train_x,train_y) #creating a frequency dictionary"
      ],
      "metadata": {
        "id": "OLvfNtzbzQcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWSqQPWrzhkU",
        "outputId": "4080665a-d53d-4f95-9813-77357fce9c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 8001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the model\n",
        "print('This is an example of positive tweet: \\n',train_x[22])\n",
        "print('\\n This is an example of processed version of the tweet:\\n',process_tweet(train_x[22]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTM97z2Pz6DM",
        "outputId": "763b65a7-188b-49dd-d8d2-7e4fdb8fa3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of positive tweet: \n",
            " @gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))\n",
            "\n",
            " This is an example of processed version of the tweet:\n",
            " ['yeah', 'suppos', 'lol', 'chat', 'bit', 'x', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#buiding the sigmoid function\n",
        "def sigmoid(z):\n",
        "  zz=np.negative(z)\n",
        "  h=1/(1+np.exp(zz))\n",
        "  return h"
      ],
      "metadata": {
        "id": "_0xwzBmm0pEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cost function implementation and gradient descent\n",
        "def gradientDescent(x,y,theta,alpha,num_iters):\n",
        "  m=x.shape[0]\n",
        "  for i in range(0,num_iters):\n",
        "    z=np.dot(x,theta)\n",
        "    h=sigmoid(z)\n",
        "    cost=-1/m*(np.dot(y.transpose(),np.log(h))+np.dot((1-y).transpose(),np.log(1-h)))\n",
        "    theta=theta-(alpha/m)*np.dot(x.transpose(),(h-y))\n",
        "  cost=float(cost)\n",
        "  return cost,theta"
      ],
      "metadata": {
        "id": "vh8zzxkt1Esx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(tweet,freqs):\n",
        "  word_1=process_tweet(tweet)\n",
        "  x=np.zeros((1,3))\n",
        "  x[0,0]=1 #bias term is set to 1\n",
        "  for word in word_1:\n",
        "    x[0,1]=x[0,1]+freqs.get((word,1.0),0) #increment the word count for positive label 1\n",
        "    x[0,2]=x[0,2]+freqs.get((word,0.0),0) #increment the word count for negative label 0\n",
        "  assert(x.shape==(1,3))\n",
        "  return x"
      ],
      "metadata": {
        "id": "KGrsN2F73F_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing on training data\n",
        "tmp1=extract_features(train_x[22],freqs)\n",
        "print(tmp1) #The three numbers obtained in the output are the feature set that we build during the build_freq and extract_features function\n",
        "#build_freq builds a dictionary having words as keys and the no of times they have occured in the corpus as values\n",
        "#extract_features take a sum of these values for positive and negative words i.e tmp1[1] and tmp[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uktqRNgsCnuB",
        "outputId": "56fba1d8-01ba-42a3-943a-913263e43b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.000e+00 3.006e+03 3.200e+01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "x=np.zeros((len(train_x),3)) #collecting features of x and stacking them into matrix 'x'\n",
        "for i in range(len(train_x)):\n",
        "  x[i, :]=extract_features(train_x[i],freqs)\n",
        "y=train_y #training labels corresponding to x\n",
        "J,theta=gradientDescent(x,y,np.zeros((3,1)),1e-9,1500) #applying gradient descent\n",
        "#these values are predefined"
      ],
      "metadata": {
        "id": "NBgGJRjzC4vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet,freqs,theta):\n",
        "  #input:\n",
        "  #tweet:a string\n",
        "  #freqs: a dictionary corresponding to the feature of each tuple (word,label)\n",
        "  #theta: a (3,1) vector of weights\n",
        "  #output:\n",
        "  #y_pred: the probability of a tweet being positive or negative\n",
        "  x=extract_features(tweet,freqs) #extracting the features of tweet and storing it into x\n",
        "  y_pred=sigmoid(np.dot(x,theta))\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "Wu24EUywFxB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression(test_x,test_y,freqs,theta):\n",
        "  y_hat=[] #the list for storing predictions\n",
        "  for tweet in test_x:\n",
        "    y_pred=predict_tweet(tweet,freqs,theta)\n",
        "    if y_pred>0.5:\n",
        "      y_hat.append(1)\n",
        "    else:\n",
        "      y_hat.append(0)\n",
        "  accuracy=(y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "dQRzNV0FGurU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy=test_logistic_regression(test_x,test_y,freqs,theta)\n",
        "print(f'Logistic regression models accuracy={tmp_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTmnY65rQ5uO",
        "outputId": "ef4b549a-bbe5-4433-a843-ee12070cb39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression models accuracy=0.7780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting with your own tweet\n",
        "def pre(sentence):\n",
        "  yhat=predict_tweet(sentence,freqs,theta)\n",
        "  if yhat>0.5:\n",
        "    return 'positive sentiment'\n",
        "  elif yhat==0:\n",
        "    return 'neutral sentiment'\n",
        "  else:\n",
        "    return 'negative sentiment'"
      ],
      "metadata": {
        "id": "VEplzh3iVS3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tweet='It is so hot today but it is the perfect day for a beach party'\n",
        "res=pre(my_tweet)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDY6PQGvWWQa",
        "outputId": "a32f71fb-25d7-46c0-d570-d872aecf4648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70Sn7B_SWmVg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}